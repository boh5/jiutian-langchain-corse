{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c40736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b739ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programs\\for_coding\\Anaconda3\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"deepseek-v3\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56466804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"你叫小智，是一名乐于助人的助手.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838a8a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='你好，我叫小明，好久不见。'),\n",
       " AIMessage(content='你好呀！我叫小智，一名乐于助人的AI助手。很高兴认识你！'),\n",
       " HumanMessage(content='你好，请问我叫什么名字。')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = [\n",
    "    HumanMessage(content=\"你好，我叫小明，好久不见。\"),\n",
    "    AIMessage(content=\"你好呀！我叫小智，一名乐于助人的AI助手。很高兴认识你！\"),\n",
    "]\n",
    "\n",
    "question = \"你好，请问我叫什么名字。\"\n",
    "\n",
    "message_list.append(HumanMessage(content=question))\n",
    "\n",
    "message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dbe396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哈哈，你刚刚说过啦！你叫小明～看来我们真的是“好久不见”呢，需要我帮你记点什么吗？ 😄\n"
     ]
    }
   ],
   "source": [
    "result = basic_qa_chain.invoke({\"messages\": message_list})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f564ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 exit 结束对话\n"
     ]
    }
   ],
   "source": [
    "chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "message_list = []\n",
    "print(\"输入 exit 结束对话\")\n",
    "while True:\n",
    "    user_query = input(\"你：\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    message_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    assistant_reply = chain.invoke({\"messages\": message_list})\n",
    "    print(\"小智：\", assistant_reply)\n",
    "\n",
    "    message_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    message_list = message_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af90c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"deepseek-v3\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "system_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你叫小智，是一名乐于助人的助手。\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = system_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf353c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bohua\\AppData\\Local\\Temp\\ipykernel_29100\\3965265779.py:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://localhost:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2229, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1752, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 785, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 890, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\bohua\\AppData\\Local\\Temp\\ipykernel_29100\\3965265779.py\", line 47, in respond\n",
      "    messages_list.append(HumanMessage(content=user_msg))\n",
      "                         ^^^^^^^^^^^^\n",
      "NameError: name 'HumanMessage' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "async def chat_response(message, history):\n",
    "    partial_message = \"\"\n",
    "\n",
    "    async for chunk in qa_chain.astream({\"input\": message}):\n",
    "        partial_message += chunk\n",
    "        yield partial_message\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2. Gradio 组件\n",
    "# ──────────────────────────────────────────────\n",
    "CSS = \"\"\"\n",
    ".main-container {max-width: 1200px; margin: 0 auto; padding: 20px;}\n",
    ".header-text {text-align: center; margin-bottom: 20px;}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_chatbot() -> gr.Blocks:\n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=CSS) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            gr.Markdown(\n",
    "                \"# 🤖 LangChain B站公开课 By九天Hector\", elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"基于 LangChain LCEL 构建的流式对话机器人\", elem_classes=[\"header-text\"]\n",
    "            )\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\",\n",
    "                ),\n",
    "            )\n",
    "            msg = gr.Textbox(placeholder=\"请输入您的问题...\", container=False, scale=7)\n",
    "            submit = gr.Button(\"发送\", scale=1, variant=\"primary\")\n",
    "            clear = gr.Button(\"清空\", scale=1)\n",
    "\n",
    "        # ---------------  状态：保存 messages_list  ---------------\n",
    "        state = gr.State([])  # 这里存放真正的 Message 对象列表\n",
    "\n",
    "        # ---------------  主响应函数（流式） ----------------------\n",
    "        async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "            # 1) 输入为空直接返回\n",
    "            if not user_msg.strip():\n",
    "                yield \"\", chat_hist, messages_list\n",
    "                return\n",
    "\n",
    "            # 2) 追加用户消息\n",
    "            messages_list.append(HumanMessage(content=user_msg))\n",
    "            chat_hist = chat_hist + [(user_msg, None)]\n",
    "            yield \"\", chat_hist, messages_list  # 先显示用户消息\n",
    "\n",
    "            # 3) 流式调用模型\n",
    "            partial = \"\"\n",
    "            async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "                partial += chunk\n",
    "                # 更新最后一条 AI 回复\n",
    "                chat_hist[-1] = (user_msg, partial)\n",
    "                yield \"\", chat_hist, messages_list\n",
    "\n",
    "            # 4) 完整回复加入历史，裁剪到最近 50 条\n",
    "            messages_list.append(AIMessage(content=partial))\n",
    "            messages_list = messages_list[-50:]\n",
    "\n",
    "            # 5) 最终返回（Gradio 需要把新的 state 传回）\n",
    "            yield \"\", chat_hist, messages_list\n",
    "\n",
    "        # ---------------  清空函数 -------------------------------\n",
    "        def clear_history():\n",
    "            return [], \"\", []  # 清空 Chatbot、输入框、messages_list\n",
    "\n",
    "        # ---------------  事件绑定 ------------------------------\n",
    "        msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3. 启动应用\n",
    "# ──────────────────────────────────────────────\n",
    "demo = create_chatbot()\n",
    "demo.launch(server_name=\"localhost\", server_port=7860, share=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
