{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c40736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b739ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programs\\for_coding\\Anaconda3\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"deepseek-v3\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56466804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838a8a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ä½ å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œå¥½ä¹…ä¸è§ã€‚'),\n",
       " AIMessage(content='ä½ å¥½å‘€ï¼æˆ‘å«å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼'),\n",
       " HumanMessage(content='ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = [\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œå¥½ä¹…ä¸è§ã€‚\"),\n",
    "    AIMessage(content=\"ä½ å¥½å‘€ï¼æˆ‘å«å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼\"),\n",
    "]\n",
    "\n",
    "question = \"ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚\"\n",
    "\n",
    "message_list.append(HumanMessage(content=question))\n",
    "\n",
    "message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dbe396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ˆå“ˆï¼Œä½ åˆšåˆšè¯´è¿‡å•¦ï¼ä½ å«å°æ˜ï½çœ‹æ¥æˆ‘ä»¬çœŸçš„æ˜¯â€œå¥½ä¹…ä¸è§â€å‘¢ï¼Œéœ€è¦æˆ‘å¸®ä½ è®°ç‚¹ä»€ä¹ˆå—ï¼Ÿ ğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "result = basic_qa_chain.invoke({\"messages\": message_list})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f564ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥ exit ç»“æŸå¯¹è¯\n"
     ]
    }
   ],
   "source": [
    "chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "message_list = []\n",
    "print(\"è¾“å…¥ exit ç»“æŸå¯¹è¯\")\n",
    "while True:\n",
    "    user_query = input(\"ä½ ï¼š\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    message_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    assistant_reply = chain.invoke({\"messages\": message_list})\n",
    "    print(\"å°æ™ºï¼š\", assistant_reply)\n",
    "\n",
    "    message_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    message_list = message_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af90c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"deepseek-v3\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "system_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = system_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf353c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bohua\\AppData\\Local\\Temp\\ipykernel_29100\\3965265779.py:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://localhost:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2229, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1752, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 785, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Files\\codes\\python_projects\\jiutian-langchain-corse\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 890, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\bohua\\AppData\\Local\\Temp\\ipykernel_29100\\3965265779.py\", line 47, in respond\n",
      "    messages_list.append(HumanMessage(content=user_msg))\n",
      "                         ^^^^^^^^^^^^\n",
      "NameError: name 'HumanMessage' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "async def chat_response(message, history):\n",
    "    partial_message = \"\"\n",
    "\n",
    "    async for chunk in qa_chain.astream({\"input\": message}):\n",
    "        partial_message += chunk\n",
    "        yield partial_message\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Gradio ç»„ä»¶\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSS = \"\"\"\n",
    ".main-container {max-width: 1200px; margin: 0 auto; padding: 20px;}\n",
    ".header-text {text-align: center; margin-bottom: 20px;}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_chatbot() -> gr.Blocks:\n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=CSS) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            gr.Markdown(\n",
    "                \"# ğŸ¤– LangChain Bç«™å…¬å¼€è¯¾ Byä¹å¤©Hector\", elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"åŸºäº LangChain LCEL æ„å»ºçš„æµå¼å¯¹è¯æœºå™¨äºº\", elem_classes=[\"header-text\"]\n",
    "            )\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\",\n",
    "                ),\n",
    "            )\n",
    "            msg = gr.Textbox(placeholder=\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\", container=False, scale=7)\n",
    "            submit = gr.Button(\"å‘é€\", scale=1, variant=\"primary\")\n",
    "            clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "\n",
    "        # ---------------  çŠ¶æ€ï¼šä¿å­˜ messages_list  ---------------\n",
    "        state = gr.State([])  # è¿™é‡Œå­˜æ”¾çœŸæ­£çš„ Message å¯¹è±¡åˆ—è¡¨\n",
    "\n",
    "        # ---------------  ä¸»å“åº”å‡½æ•°ï¼ˆæµå¼ï¼‰ ----------------------\n",
    "        async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "            # 1) è¾“å…¥ä¸ºç©ºç›´æ¥è¿”å›\n",
    "            if not user_msg.strip():\n",
    "                yield \"\", chat_hist, messages_list\n",
    "                return\n",
    "\n",
    "            # 2) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "            messages_list.append(HumanMessage(content=user_msg))\n",
    "            chat_hist = chat_hist + [(user_msg, None)]\n",
    "            yield \"\", chat_hist, messages_list  # å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯\n",
    "\n",
    "            # 3) æµå¼è°ƒç”¨æ¨¡å‹\n",
    "            partial = \"\"\n",
    "            async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "                partial += chunk\n",
    "                # æ›´æ–°æœ€åä¸€æ¡ AI å›å¤\n",
    "                chat_hist[-1] = (user_msg, partial)\n",
    "                yield \"\", chat_hist, messages_list\n",
    "\n",
    "            # 4) å®Œæ•´å›å¤åŠ å…¥å†å²ï¼Œè£å‰ªåˆ°æœ€è¿‘ 50 æ¡\n",
    "            messages_list.append(AIMessage(content=partial))\n",
    "            messages_list = messages_list[-50:]\n",
    "\n",
    "            # 5) æœ€ç»ˆè¿”å›ï¼ˆGradio éœ€è¦æŠŠæ–°çš„ state ä¼ å›ï¼‰\n",
    "            yield \"\", chat_hist, messages_list\n",
    "\n",
    "        # ---------------  æ¸…ç©ºå‡½æ•° -------------------------------\n",
    "        def clear_history():\n",
    "            return [], \"\", []  # æ¸…ç©º Chatbotã€è¾“å…¥æ¡†ã€messages_list\n",
    "\n",
    "        # ---------------  äº‹ä»¶ç»‘å®š ------------------------------\n",
    "        msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. å¯åŠ¨åº”ç”¨\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "demo = create_chatbot()\n",
    "demo.launch(server_name=\"localhost\", server_port=7860, share=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
