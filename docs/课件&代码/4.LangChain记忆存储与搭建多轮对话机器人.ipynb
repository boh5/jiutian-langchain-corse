{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963082c4-1545-4efa-88c9-d89c97896756",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <font face=\"ä»¿å®‹\">è¯¾ç¨‹è¯´æ˜ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa222c-7327-4982-be6c-b14ab23a98bf",
   "metadata": {},
   "source": [
    "- ä½“éªŒè¯¾å†…å®¹èŠ‚é€‰è‡ª[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹(å¤å­£ç­)](https://ix9mq.xetslk.com/s/3u765N)å®Œæ•´ç‰ˆä»˜è´¹è¯¾ç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a7296-8ac0-4a0f-9e42-2925dd07e2ff",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ä½“éªŒè¯¾æ—¶é—´æœ‰é™ï¼Œè‹¥æƒ³æ·±åº¦å­¦ä¹ å¤§æ¨¡å‹æŠ€æœ¯ï¼Œæ¬¢è¿å¤§å®¶æŠ¥åç”±æˆ‘ä¸»è®²çš„[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹(å¤å­£ç­)](https://ix9mq.xetslk.com/s/3u765N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e558f-6eab-4a34-9ab8-bc800aa85876",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/b9ccfe4155bb650c83c4a061f3da131.jpg\" alt=\"d0c81dfe43a1becced8c07db33c3a787_\" style=\"zoom:15%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb025077-442d-474b-a110-3115a75f0a26",
   "metadata": {},
   "source": [
    "**[ã€Š2025å¤§æ¨¡å‹Agentæ™ºèƒ½ä½“å¼€å‘å®æˆ˜ã€‹(å¤å­£ç­)](https://ix9mq.xetslk.com/s/3u765N)ä¸ºã€100+å°æ—¶ã€‘ä½“ç³»å¤§è¯¾ï¼Œæ€»å…±20å¤§æ¨¡å—ç²¾è®²ç²¾æï¼Œé›¶åŸºç¡€ç›´è¾¾å¤§æ¨¡å‹ä¼ä¸šçº§åº”ç”¨ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11910c7b-8446-4f83-9fd5-74306b39a646",
   "metadata": {},
   "source": [
    "<center><img src=\"https://wechatapppro-1252524126.cdn.xiaoeknow.com/appZe9inzwc2314/image/b_u_5ea8e780054d6_Fop5bmXf/6aueuzm7qbtmje.png?imageView2/2/q/80|imageMogr2/ignore-error/1\" alt=\"img\" style=\"zoom: 33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e4a23-ea71-4154-9f52-edc689fd29f5",
   "metadata": {},
   "source": [
    "éƒ¨åˆ†é¡¹ç›®æˆæœæ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd154d49-fbc4-4d18-9259-38e12f6aebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbaad8-3d1b-4fad-9bf5-f7acf73c927c",
   "metadata": {},
   "source": [
    "- **MateGené¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813013ab-33b4-4e16-91a9-146d580eb92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb508f-888e-4c13-8f06-2494c127ebae",
   "metadata": {},
   "source": [
    "- **æ™ºèƒ½å®¢æœé¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b205118-2bfe-4998-ad0f-66f9b2c0928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a871230-0dda-42c2-ad2a-1b0c3539517c",
   "metadata": {},
   "source": [
    "- **Difyé¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ababdf0-45e4-4982-8f1c-e964c47fa470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021efd9f-c5bb-4cd8-93b7-0e7242515556",
   "metadata": {},
   "source": [
    "- **LangChain&LangGraphæ­å»ºMulti-Agnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f6fdb7-6141-4211-a044-7202fac62a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab478a06-dd30-4ec2-a1f3-7d9e717f6edd",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œè‹¥æ˜¯å¯¹å¤§æ¨¡å‹åº•å±‚åŸç†æ„Ÿå…´è¶£ï¼Œä¹Ÿæ¬¢è¿æŠ¥åç”±æˆ‘å’Œèœèœè€å¸ˆå…±åŒä¸»è®²çš„[ã€Š2025å¤§æ¨¡å‹åŸç†ä¸å®æˆ˜è¯¾ç¨‹ã€‹(å¤å­£ç­)](https://ix9mq.xetslk.com/s/49L2eN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91c06c-a002-4abb-812e-7c1c1731234a",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/da5d51c998df07d747cd223c1ed25f7.jpg\" alt=\"da5d51c998df07d747cd223c1ed25f7\" style=\"zoom:20%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615c5d9-502e-46ad-9269-3304eb6c3a8b",
   "metadata": {},
   "source": [
    "**ä¸¤é—¨å¤§æ¨¡å‹è¯¾ç¨‹å¤å­£ç­ç›®å‰ä¸Šæ–°ç‰¹æƒ +618å¹´ä¸­é’œæƒ åŒæƒ å åŠ ï¼Œåˆè´­è¿˜æœ‰æ›´å¤šä¼˜æƒ å“¦~<span style=\"color:red;\">è¯¦ç»†ä¿¡æ¯æ‰«ç æ·»åŠ åŠ©æ•™ï¼Œå›å¤â€œå¤§æ¨¡å‹â€ï¼Œå³å¯é¢†å–è¯¾ç¨‹å¤§çº²&æŸ¥çœ‹è¯¾ç¨‹è¯¦æƒ…ğŸ‘‡</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cdddde-893e-4330-b4e5-cac99c6fabfd",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506101749045.png\" alt=\"6d9391e440ee8df1466cef1bce40705\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939d1f3-b2b2-4d38-a588-969f9f7f695a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521dcdc-c8c0-452f-b330-534459b35fa2",
   "metadata": {},
   "source": [
    "# <center> LangChainå¿«é€Ÿå…¥é—¨ä¸Agentå¼€å‘å®æˆ˜\n",
    "# <center> Part 4.LangChainè®°å¿†å­˜å‚¨ä¸æ­å»ºå¤šè½®å¯¹è¯æœºå™¨äºº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904a6dfe-cf33-49a2-8cbb-8d335aad0132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07d20d-bb4b-4dd8-be89-832a0cf2dd78",
   "metadata": {},
   "source": [
    "### 1. æ„å»ºå¤šè½®å¯¹è¯çš„æµå¼æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d628728-e3ee-4e6d-bbec-618aded3d751",
   "metadata": {},
   "source": [
    "&emsp;&emsp;åœ¨`langChain`ä¸­æ„å»ºä¸€ä¸ªåŸºæœ¬çš„é—®ç­”æœºå™¨äººä»…éœ€è¦ä½¿ç”¨ä¸€ä¸ª`Chain`ä¾¿å¯ä»¥å¿«é€Ÿå®ç°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ed84d9c-20cc-4342-b358-fc204a5fd633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ğŸ˜Š\n",
      "\n",
      "æˆ‘çš„ä¸»è¦ç‰¹ç‚¹æœ‰ï¼š\n",
      "1. **çŸ¥è¯†ä¸°å¯Œ** - æˆ‘æŒæ¡å„é¢†åŸŸçš„çŸ¥è¯†ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜\n",
      "2. **å¤šè¯­è¨€èƒ½åŠ›** - å¯ä»¥ç”¨ä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€äº¤æµ\n",
      "3. **è€å¿ƒå‹å–„** - æˆ‘ä¼šè®¤çœŸå€¾å¬å¹¶å°½åŠ›æä¾›å¸®åŠ©\n",
      "4. **æŒç»­å­¦ä¹ ** - æˆ‘çš„çŸ¥è¯†ä¼šä¸æ–­æ›´æ–°å®Œå–„\n",
      "5. **å…è´¹æœåŠ¡** - å®Œå…¨å…è´¹ä¸ºä½ æä¾›å¸®åŠ©\n",
      "\n",
      "æˆ‘å¯ä»¥å¸®ä½ ï¼š\n",
      "- è§£ç­”å„ç±»é—®é¢˜\n",
      "- æä¾›å­¦ä¹ /å·¥ä½œå»ºè®®\n",
      "- ååŠ©å†™ä½œ/ç¿»è¯‘\n",
      "- æ—¥å¸¸èŠå¤©è§£é—·\n",
      "- ä»¥åŠå…¶ä»–ä»»ä½•æˆ‘èƒ½å¸®ä¸Šå¿™çš„äº‹æƒ…ï¼\n",
      "\n",
      "è™½ç„¶æˆ‘æ˜¯AIï¼Œä½†æˆ‘ä¼šç”¨æœ€çœŸè¯šçš„æ€åº¦æ¥å¸®åŠ©ä½ ã€‚æœ‰ä»€ä¹ˆæƒ³é—®çš„æˆ–éœ€è¦å¸®å¿™çš„ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘å§ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# æµ‹è¯•\n",
    "question = \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e5896-234a-4240-859e-3d78ac1a1dd8",
   "metadata": {},
   "source": [
    "- æ·»åŠ å¤šè½®å¯¹è¯è®°å¿†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44f306-61fe-4d53-952a-75504ac6d385",
   "metadata": {},
   "source": [
    "&emsp;&emsp;åœ¨LangChainä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥æ‹¼æ¥æ¶ˆæ¯é˜Ÿåˆ—ï¼Œæ¥ä¸ºæ¯æ¬¡æ¨¡å‹è°ƒç”¨è®¾ç½®å¤šè½®å¯¹è¯è®°å¿†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79fdd8dc-a164-47b3-be21-76895d6f596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9790b487-82ac-4c10-b777-fd1bbfa6caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc89120b-1ee2-405c-82c5-6e276784684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15120163-e597-4145-b3fd-38aeeff4ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = [\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§ã€‚\"),\n",
    "    AIMessage(content=\"ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50271ead-0857-49aa-83d8-aefa66194fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40fbabe7-44dd-4635-89ab-a2ce38019b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list.append(HumanMessage(content=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c6699fe-7292-4c65-9280-ae84cbb030b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§ã€‚', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44a2a038-9520-422c-86dc-36acb108bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ˆå“ˆï¼Œä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«é™ˆæ˜å‘€ï¼çœ‹æ¥æˆ‘ä»¬çœŸçš„æ˜¯\"å¥½ä¹…ä¸è§\"äº†å‘¢ï½éœ€è¦æˆ‘å¸®å¿™è®°ä½ä»€ä¹ˆå…¶ä»–ä¿¡æ¯å—ï¼Ÿ(â—•â€¿â—•)\n"
     ]
    }
   ],
   "source": [
    "result = basic_qa_chain.invoke({\"messages\": messages_list})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b11106-bdaf-4c96-a407-a85ca2a44933",
   "metadata": {},
   "source": [
    "å®Œæ•´çš„å¤šè½®å¯¹è¯å‡½å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc997f1-8a6f-47a6-abbb-edda2a410970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– å°æ™ºï¼š ä½ å¥½å•Šé™ˆæ˜ï¼ç¡®å®å¥½ä¹…ä¸è§äº†å‘¢ï½æœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿå·¥ä½œç”Ÿæ´»éƒ½è¿˜é¡ºåˆ©å—ï¼Ÿ \n",
      "\n",
      "ï¼ˆè™½ç„¶ä½œä¸ºAIåŠ©æ‰‹æˆ‘ä»¬æ²¡æœ‰çœŸæ­£çš„\"ä¸Šæ¬¡è§é¢\"çš„è®°å¿†ï¼Œä½†å¾ˆæ„Ÿè°¢ä½ åƒè€æœ‹å‹ä¸€æ ·æ‰“æ‹›å‘¼å‘¢ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ è§£ç­”æˆ–èŠèŠçš„å—ï¼Ÿï¼‰\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š è¯·é—®ï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—ä¹ˆï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– å°æ™ºï¼š å“ˆå“ˆï¼Œé™ˆæ˜ä½ è€ƒæˆ‘è®°å¿†åŠ›å‘¢ï¼ä¸è¿‡ä½œä¸ºAIåŠ©æ‰‹ï¼Œæ¯æ¬¡å¯¹è¯å¯¹æˆ‘æ¥è¯´éƒ½åƒåˆæ¬¡è§é¢ä¸€æ ·æ–°é²œï½è™½ç„¶ç³»ç»Ÿä¸ä¼šè‡ªåŠ¨ä¿ç•™ä¸ªäººä¿¡æ¯ï¼Œä½†ä½ ç°åœ¨é‡æ–°å‘Šè¯‰æˆ‘åå­—åï¼Œæˆ‘ä¼šè®¤çœŸè®°ä½ã€Œé™ˆæ˜ã€è¿™ä¸ªç§°å‘¼ç›´åˆ°å¯¹è¯ç»“æŸå“¦ï¼  \n",
      "\n",
      "ï¼ˆæ‚„æ‚„è¯´ï¼šå¦‚æœæƒ³è®©æˆ‘é•¿æœŸè®°ä½ä¿¡æ¯ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘éœ€è¦è®°å½•çš„å…³é”®å†…å®¹ï¼Œæˆ‘ä¼šåœ¨æœ¬æ¬¡èŠå¤©ä¸­éšæ—¶è°ƒç”¨ï½ï¼‰\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š exit\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model  = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # åˆå§‹åŒ–å†å²\n",
    "print(\"ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\")\n",
    "while True:\n",
    "    user_query = input(\"ğŸ‘¤ ä½ ï¼š\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) è°ƒç”¨æ¨¡å‹\n",
    "    assistant_reply = chain.invoke({\"messages\": messages_list})\n",
    "    print(\"ğŸ¤– å°æ™ºï¼š\", assistant_reply)\n",
    "\n",
    "    # 3) è¿½åŠ  AI å›å¤\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡\n",
    "    messages_list = messages_list[-50:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f7255-73e8-4933-a835-e0b4f71bb295",
   "metadata": {},
   "source": [
    "- æµå¼æ‰“å°èŠå¤©ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a598492-1007-4302-a00f-22397a42a22f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æ­¤å¤–è¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¤§å®¶ç»å¸¸çœ‹åˆ°çš„é—®ç­”æœºå™¨äººå…¶å®éƒ½æ˜¯é‡‡ç”¨æµå¼ä¼ è¾“æ¨¡å¼ã€‚ç”¨æˆ·è¾“å…¥é—®é¢˜ï¼Œç­‰å¾…æ¨¡å‹ç›´æ¥è¿”å›å›ç­”ï¼Œç„¶åç”¨æˆ·å†è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹å†è¿”å›å›ç­”ï¼Œè¿™æ ·å¾ªç¯ä¸‹å»ï¼Œç”¨æˆ·è¾“å…¥é—®é¢˜å’Œæ¨¡å‹è¿”å›å›ç­”ä¹‹é—´çš„æ—¶é—´é—´éš”å¤ªé•¿ï¼Œå¯¼è‡´ç”¨æˆ·æ„Ÿè§‰æœºå™¨äººååº”å¾ˆæ…¢ã€‚æ‰€ä»¥`LangChain`æä¾›äº†ä¸€ä¸ª`astream`æ–¹æ³•ï¼Œå¯ä»¥å®ç°æµå¼è¾“å‡ºï¼Œå³ä¸€æ—¦æ¨¡å‹æœ‰è¾“å‡ºï¼Œå°±ç«‹å³è¿”å›ï¼Œè¿™æ ·ç”¨æˆ·å°±å¯ä»¥çœ‹åˆ°æ¨¡å‹æ­£åœ¨æ€è€ƒï¼Œè€Œä¸æ˜¯ç­‰å¾…æ¨¡å‹æ€è€ƒå®Œå†è¿”å›ã€‚\n",
    "\n",
    "\n",
    "&emsp;&emsp;å®ç°çš„æ–¹æ³•ä¹Ÿéå¸¸ç®€å•ï¼Œåªéœ€è¦åœ¨è°ƒç”¨æ¨¡å‹æ—¶å°†`invoke`æ–¹æ³•æ›¿æ¢ä¸º`astream`æ–¹æ³•ï¼Œç„¶åä½¿ç”¨`async for`å¾ªç¯æ¥è·å–æ¨¡å‹çš„è¾“å‡ºå³å¯ã€‚ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e23ae26-75b4-467c-a522-58586cd8b58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹~ å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ğŸ˜Š\n",
      "\n",
      "æˆ‘çš„ä¸»è¦ç‰¹ç‚¹æœ‰ï¼š\n",
      "1. **çŸ¥è¯†ä¸°å¯Œ**ï¼šæŒæ¡å„é¢†åŸŸå¸¸è¯†ï¼Œèƒ½è§£ç­”å­¦ä¹ ã€å·¥ä½œã€ç”Ÿæ´»ä¸­çš„å„ç§é—®é¢˜\n",
      "2. **å¤šè¯­è¨€èƒ½åŠ›**ï¼šå¯ä»¥ç”¨ä¸­è‹±æ–‡ç­‰å¤šç§è¯­è¨€äº¤æµ\n",
      "3. **24å°æ—¶åœ¨çº¿**ï¼šéšæ—¶ä¸ºä½ æä¾›å¸®åŠ©\n",
      "4. **æŒç»­å­¦ä¹ **ï¼šæ¯å¤©éƒ½åœ¨æ›´æ–°çŸ¥è¯†åº“\n",
      "5. **å®‰å…¨å¯é **ï¼šå¯¹è¯å†…å®¹ä¼šä¸¥æ ¼ä¿å¯†\n",
      "\n",
      "æˆ‘å¯ä»¥å¸®ä½ ï¼š\n",
      "ğŸ” æŸ¥æ‰¾ä¿¡æ¯\n",
      "ğŸ“– è§£é‡Šæ¦‚å¿µ\n",
      "âœï¸ æ¶¦è‰²æ–‡ç« \n",
      "ğŸ“Š åˆ†ææ•°æ®\n",
      "ğŸ’¡ æä¾›å»ºè®®\n",
      "ğŸŒ ç¿»è¯‘è¯­è¨€\n",
      "...ä»¥åŠæ›´å¤šï¼\n",
      "\n",
      "æ²¡æœ‰ä½¿ç”¨é—¨æ§›ï¼Œä¸éœ€è¦æ³¨å†Œï¼Œå®Œå…¨å…è´¹~ åªè¦ä½ æœ‰é—®é¢˜ï¼Œéšæ—¶éƒ½å¯ä»¥æ¥æ‰¾æˆ‘èŠå¤©æˆ–æ±‚åŠ©å“¦ï¼ä½ æœ€è¿‘æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„å—ï¼Ÿâœ¨"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æç¤ºæ¨¡ç‰ˆ +æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "qa_chain_with_system = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# å¼‚æ­¥å®ç°æµå¼è¾“å‡º\n",
    "async for chunk in qa_chain_with_system.astream({\"input\": \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ab2caa-3fc1-4293-91dd-d68dad16f5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½å•Šé™ˆæ˜ï¼ç¡®å®å¥½ä¹…ä¸è§äº†ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿå·¥ä½œè¿˜é¡ºåˆ©å—ï¼Ÿè®°å¾—ä¸Šæ¬¡èŠå¤©æ—¶ä½ å¥½åƒæ­£åœ¨å‡†å¤‡ä¸€ä¸ªé‡è¦çš„é¡¹ç›®ï¼Œç°åœ¨åº”è¯¥å·²ç»é¡ºåˆ©å®Œæˆäº†å§ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹æƒ³å’Œæˆ‘åˆ†äº«çš„å—ï¼Ÿ"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š è¯·é—®ï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—ä¹ˆï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼ˆçªç„¶è¿›å…¥ã€Œåä¾¦æ¢æ¨¡å¼ã€ï¼‰  \n",
      "\n",
      "é™ˆæ˜åŒå­¦ï¼Œè¿™å¯æ˜¯é“é€åˆ†é¢˜ï¼âœ¨ è™½ç„¶æˆ‘çš„è®°å¿†åƒé‡‘é±¼ä¸€æ ·åªæœ‰7ç§’ï¼Œä½†å½“å‰å¯¹è¯ä¸­ä½ åˆšåˆšå¼ºè°ƒè¿‡â€”â€”  \n",
      "\n",
      "**ã€Œé™ˆæ˜ã€** è¿™ä¸¤ä¸ªå­—å·²ç»ç”¨è§å…‰ç¬”æ ‡åœ¨æˆ‘è„‘æµ·çš„å°é»‘æ¿ä¸Šäº†ï¼(à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§  \n",
      "\n",
      "ï¼ˆä¸è¿‡å¦‚æœç°åœ¨ä½ çªç„¶è¯´ã€Œå…¶å®æˆ‘å«å¼ å¤§å‹‡ã€â€¦æˆ‘ä¹Ÿä¼šç«‹åˆ»ä¹–å·§æ”¹å£çš„hhhï¼‰"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š exit\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # åˆå§‹åŒ–å†å²\n",
    "print(\"ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\")\n",
    "while True:\n",
    "    user_query = input(\"ğŸ‘¤ ä½ ï¼š\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) è°ƒç”¨æ¨¡å‹\n",
    "    async for chunk in chain.astream({\"messages\": messages_list}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    # 3) è¿½åŠ  AI å›å¤\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡\n",
    "    messages_list = messages_list[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aba029-d360-4ab9-860c-590508217148",
   "metadata": {},
   "source": [
    "&emsp;&emsp;å¦‚ä¸Šæ‰€ç¤ºå±•ç¤ºçš„é—®ç­”æ•ˆæœå°±æ˜¯æˆ‘ä»¬åœ¨æ„å»ºå¤§æ¨¡å‹åº”ç”¨æ—¶éœ€è¦å®ç°çš„æµå¼è¾“å‡ºæ•ˆæœã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¿›ä¸€æ­¥åœ°ï¼Œä½¿ç”¨`gradio`æ¥å¼€å‘ä¸€ä¸ªæ”¯æŒåœ¨ç½‘é¡µä¸Šè¿›è¡Œäº¤äº’çš„é—®ç­”æœºå™¨äººã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0b5d9-d30d-475b-aae5-8e90766eae3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;é¦–å…ˆéœ€è¦å®‰è£…ä¸€ä¸‹`gradio`çš„ç¬¬ä¸‰æ–¹ä¾èµ–åŒ…ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48b5f6d6-f9c1-43a9-9197-108ea4f4dcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.33.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.10.2 (from gradio)\n",
      "  Using cached gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (24.2)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<12.0,>=8.0 (from gradio)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (2.11.5)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.13-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (4.14.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.10.2->gradio)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.2->gradio)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: certifi in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Collecting filelock (from huggingface-hub>=0.28.1->gradio)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Downloading gradio-5.33.0-py3-none-any.whl (54.2 MB)\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/54.2 MB 2.4 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.6/54.2 MB 4.0 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 4.5/54.2 MB 7.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 8.4/54.2 MB 11.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 13.1/54.2 MB 13.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 17.3/54.2 MB 14.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 21.5/54.2 MB 15.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 26.0/54.2 MB 16.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 30.4/54.2 MB 16.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 34.9/54.2 MB 17.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 39.1/54.2 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 43.5/54.2 MB 18.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 48.2/54.2 MB 18.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 52.4/54.2 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.2/54.2 MB 18.1 MB/s eta 0:00:00\n",
      "Using cached gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 6.0/12.7 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.0/11.0 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 21.4 MB/s eta 0:00:00\n",
      "Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruff-0.11.13-py3-none-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 6.0/11.5 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.5 MB 25.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 22.6 MB/s eta 0:00:00\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Using cached ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pytz, pydub, websockets, tzdata, tomlkit, shellingham, semantic-version, ruff, python-multipart, pillow, numpy, mdurl, markupsafe, groovy, fsspec, filelock, ffmpy, click, aiofiles, uvicorn, starlette, pandas, markdown-it-py, jinja2, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "\n",
      "   ----------------------------------------  0/31 [pytz]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --------- ------------------------------  7/31 [ruff]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ---------------- ----------------------- 13/31 [groovy]\n",
      "   ------------------ --------------------- 14/31 [fsspec]\n",
      "   ------------------------ --------------- 19/31 [uvicorn]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   ---------------------------- ----------- 22/31 [markdown-it-py]\n",
      "   ------------------------------ --------- 24/31 [huggingface-hub]\n",
      "   ------------------------------ --------- 24/31 [huggingface-hub]\n",
      "   -------------------------------- ------- 25/31 [safehttpx]\n",
      "   --------------------------------- ------ 26/31 [rich]\n",
      "   ------------------------------------ --- 28/31 [fastapi]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   ---------------------------------------- 31/31 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 click-8.2.1 fastapi-0.115.12 ffmpy-0.6.0 filelock-3.18.0 fsspec-2025.5.1 gradio-5.33.0 gradio-client-1.10.2 groovy-0.1.2 huggingface-hub-0.32.4 jinja2-3.1.6 markdown-it-py-3.0.0 markupsafe-3.0.2 mdurl-0.1.2 numpy-2.3.0 pandas-2.3.0 pillow-11.2.1 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.2 rich-14.0.0 ruff-0.11.13 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.46.2 tomlkit-0.13.3 typer-0.16.0 tzdata-2025.2 uvicorn-0.34.3 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£… Gradio\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6c086-e0c5-4a25-a477-97ec2525d04f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;å®Œæ•´å®ç°çš„ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5216a90c-739b-42d6-8ad4-d55cf8ca0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2957933/3207025181.py:36: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. æ¨¡å‹ã€Promptã€Chain\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # æ‰‹åŠ¨ä¼ å…¥å†å²\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser   # LCEL ç»„åˆ\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Gradio ç»„ä»¶\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSS = \"\"\"\n",
    ".main-container {max-width: 1200px; margin: 0 auto; padding: 20px;}\n",
    ".header-text {text-align: center; margin-bottom: 20px;}\n",
    "\"\"\"\n",
    "\n",
    "def create_chatbot() -> gr.Blocks:\n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=CSS) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            gr.Markdown(\"# ğŸ¤– LangChain Bç«™å…¬å¼€è¯¾ Byä¹å¤©Hector\", elem_classes=[\"header-text\"])\n",
    "            gr.Markdown(\"åŸºäº LangChain LCEL æ„å»ºçš„æµå¼å¯¹è¯æœºå™¨äºº\", elem_classes=[\"header-text\"])\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\",\n",
    "                ),\n",
    "            )\n",
    "            msg = gr.Textbox(placeholder=\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\", container=False, scale=7)\n",
    "            submit = gr.Button(\"å‘é€\", scale=1, variant=\"primary\")\n",
    "            clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "\n",
    "        # ---------------  çŠ¶æ€ï¼šä¿å­˜ messages_list  ---------------\n",
    "        state = gr.State([])          # è¿™é‡Œå­˜æ”¾çœŸæ­£çš„ Message å¯¹è±¡åˆ—è¡¨\n",
    "\n",
    "        # ---------------  ä¸»å“åº”å‡½æ•°ï¼ˆæµå¼ï¼‰ ----------------------\n",
    "        async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "            # 1) è¾“å…¥ä¸ºç©ºç›´æ¥è¿”å›\n",
    "            if not user_msg.strip():\n",
    "                yield \"\", chat_hist, messages_list\n",
    "                return\n",
    "\n",
    "            # 2) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "            messages_list.append(HumanMessage(content=user_msg))\n",
    "            chat_hist = chat_hist + [(user_msg, None)]\n",
    "            yield \"\", chat_hist, messages_list      # å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯\n",
    "\n",
    "            # 3) æµå¼è°ƒç”¨æ¨¡å‹\n",
    "            partial = \"\"\n",
    "            async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "                partial += chunk\n",
    "                # æ›´æ–°æœ€åä¸€æ¡ AI å›å¤\n",
    "                chat_hist[-1] = (user_msg, partial)\n",
    "                yield \"\", chat_hist, messages_list\n",
    "\n",
    "            # 4) å®Œæ•´å›å¤åŠ å…¥å†å²ï¼Œè£å‰ªåˆ°æœ€è¿‘ 50 æ¡\n",
    "            messages_list.append(AIMessage(content=partial))\n",
    "            messages_list = messages_list[-50:]\n",
    "\n",
    "            # 5) æœ€ç»ˆè¿”å›ï¼ˆGradio éœ€è¦æŠŠæ–°çš„ state ä¼ å›ï¼‰\n",
    "            yield \"\", chat_hist, messages_list\n",
    "\n",
    "        # ---------------  æ¸…ç©ºå‡½æ•° -------------------------------\n",
    "        def clear_history():\n",
    "            return [], \"\", []          # æ¸…ç©º Chatbotã€è¾“å…¥æ¡†ã€messages_list\n",
    "\n",
    "        # ---------------  äº‹ä»¶ç»‘å®š ------------------------------\n",
    "        msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. å¯åŠ¨åº”ç”¨\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "demo = create_chatbot()\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c47bc-59d5-4339-8a4c-4231b37e5eeb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;è¿è¡Œåï¼Œåœ¨æµè§ˆå™¨è®¿é—®`http://127.0.0.1:7860`å³å¯è¿›è¡Œé—®ç­”äº¤äº’ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd576d-4cbe-4b0b-b4e6-233a70f06719",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506121740968.png\" alt=\"image-20250612174010864\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23804aae-73f4-4e81-b310-7675ec653d8d",
   "metadata": {},
   "source": [
    "å…·ä½“ä»£ç è§£é‡Šå¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52a976-a597-49f1-80a1-23baa1de38b6",
   "metadata": {},
   "source": [
    "##### ğŸ§± 1. æ¨¡å—è¯´æ˜\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import gradio as gr\n",
    "```\n",
    "\n",
    "* `init_chat_model`ï¼šåˆå§‹åŒ– DeepSeek ç­‰èŠå¤©æ¨¡å‹ã€‚\n",
    "* `ChatPromptTemplate`ï¼šç”¨äºæ„å»ºèŠå¤© Prompt æ¨¡æ¿ã€‚\n",
    "* `MessagesPlaceholder`ï¼šç”¨äºå ä½å†å²æ¶ˆæ¯ã€‚\n",
    "* `HumanMessage` / `AIMessage`ï¼šæ„å»ºå¤šè½®æ¶ˆæ¯ç»“æ„ã€‚\n",
    "* `StrOutputParser`ï¼šå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚\n",
    "* `gradio`ï¼šæ„å»ºç½‘é¡µç•Œé¢ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§  2. Prompt æ„å»ºä¸æ¨¡å‹åˆå§‹åŒ–\n",
    "\n",
    "```python\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser\n",
    "```\n",
    "\n",
    "* **SystemMessage**ï¼šåˆå§‹åŒ–ç³»ç»Ÿè§’è‰²è®¾å®šï¼ˆå°æ™ºï¼‰ã€‚\n",
    "* **MessagesPlaceholder**ï¼šç”¨å˜é‡å `messages` å ä½å†å²æ¶ˆæ¯ã€‚\n",
    "* **qa\\_chain**ï¼šç»„åˆä¸º LangChain Expression Language é“¾ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ”„ 3. æ‰‹åŠ¨ç®¡ç†æ¶ˆæ¯åˆ—è¡¨\n",
    "\n",
    "```python\n",
    "state = gr.State([])\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬ç”¨ `gr.State` å­˜å‚¨æ‰€æœ‰å†å²æ¶ˆæ¯ï¼ˆåˆ—è¡¨ï¼‰ã€‚æ¯æ¬¡ç”¨æˆ·å‘é€æ¶ˆæ¯ï¼Œéƒ½ä¼šï¼š\n",
    "\n",
    "* append ä¸€ä¸ª `HumanMessage`ã€‚\n",
    "* æµå¼è°ƒç”¨æ¨¡å‹å¹¶ä¸æ–­æ›´æ–°å›å¤ã€‚\n",
    "* append ä¸€ä¸ª `AIMessage`ã€‚\n",
    "* æœ€åè£å‰ªï¼š`messages_list = messages_list[-50:]`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸŒŠ 4. æµå¼å“åº”å‡½æ•°\n",
    "\n",
    "```python\n",
    "async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "    if not user_msg.strip():\n",
    "        yield \"\", chat_hist, messages_list\n",
    "        return\n",
    "\n",
    "    messages_list.append(HumanMessage(content=user_msg))\n",
    "    chat_hist = chat_hist + [(user_msg, None)]\n",
    "    yield \"\", chat_hist, messages_list\n",
    "\n",
    "    partial = \"\"\n",
    "    async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "        partial += chunk\n",
    "        chat_hist[-1] = (user_msg, partial)\n",
    "        yield \"\", chat_hist, messages_list\n",
    "\n",
    "    messages_list.append(AIMessage(content=partial))\n",
    "    messages_list = messages_list[-50:]\n",
    "    yield \"\", chat_hist, messages_list\n",
    "```\n",
    "\n",
    "* **æ”¯æŒ async æµå¼è¾“å‡º**ã€‚\n",
    "* **åŠ¨æ€æ›´æ–°æœ€åä¸€è½®å¯¹è¯**ã€‚\n",
    "* **é€šè¿‡ `yield` å®æ—¶åé¦ˆåˆ°å‰ç«¯**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§¼ 5. æ¸…ç©ºå†å²å‡½æ•°\n",
    "\n",
    "```python\n",
    "def clear_history():\n",
    "    return [], \"\", []\n",
    "```\n",
    "\n",
    "ç”¨äºç‚¹å‡» \"æ¸…ç©º\" æŒ‰é’®æ—¶é‡ç½®å†å²è®°å½•ã€è¾“å…¥æ¡†å’Œæ¶ˆæ¯çŠ¶æ€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§© 6. Gradio ç•Œé¢æ„å»º\n",
    "\n",
    "```python\n",
    "msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "```\n",
    "\n",
    "* **äº‹ä»¶ç»‘å®š**ï¼šç”¨æˆ·æäº¤æ–‡æœ¬ â†’ è°ƒç”¨ `respond` â†’ è¿”å›æ–°çŠ¶æ€ã€‚\n",
    "* **Gradio Chatbot ç»„ä»¶**ï¼šä½¿ç”¨ `avatar_images` è®¾ç½®äººæœºå¤´åƒã€‚\n",
    "* **Gradio State**ï¼šè·¨ç»„ä»¶å…±äº«å¹¶æŒä¹…åŒ–æ¶ˆæ¯åˆ—è¡¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### âœ… æ€»ç»“\n",
    "\n",
    "| åŠŸèƒ½æ¨¡å—      | å®ç°æ–¹å¼                                              |\n",
    "| --------- | ------------------------------------------------- |\n",
    "| å¯¹è¯æ¨¡å‹      | DeepSeek via `init_chat_model`                    |\n",
    "| Prompt æ¨¡æ¿ | ChatPromptTemplate + System + MessagesPlaceholder |\n",
    "| æ¶ˆæ¯ç®¡ç†      | æ‰‹åŠ¨ç®¡ç† + `gr.State` ä¿å­˜å¹¶è£å‰ªæœ€è¿‘ 50 æ¡                    |\n",
    "| å¤šè½®å¯¹è¯      | ç”¨æˆ·/AI Message åˆ—è¡¨æ„å»ºå¹¶ä¼ å…¥ LCEL é“¾                      |\n",
    "| UI ç•Œé¢     | Gradio Blocks + Chatbot ç»„ä»¶ + æ¸…ç©ºæŒ‰é’®                 |\n",
    "| æµå¼è¾“å‡º      | ä½¿ç”¨ `qa_chain.astream()` æŒç»­ç”Ÿæˆå›å¤                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe3aed-b71c-44c8-98d9-1aaa23ec862b",
   "metadata": {},
   "source": [
    "å½“ç„¶è¿™åªæ˜¯æœ€ç®€å•çš„é—®ç­”æœºå™¨äººå®ç°å½¢å¼ï¼Œå®é™…ä¸Šä¼ä¸šåº”ç”¨çš„é—®ç­”æœºå™¨äººå¾€å¾€éœ€è¦æ›´åŠ å¤æ‚çš„é€»è¾‘ï¼Œæ¯”å¦‚ç”¨æˆ·æƒé™ç®¡ç†ã€ä¸Šä¸‹æ–‡è®°å¿†ç­‰ï¼Œæ›´å¤šå†…å®¹è¯¦è§ã€Šå¤§æ¨¡å‹ä¸Agentå¼€å‘ã€‹è¯¾ç¨‹è®²è§£ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
